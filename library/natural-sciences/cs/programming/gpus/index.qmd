---
title: Programming GPUs
---


## Resources

- [Github of Resources](https://github.com/arpitingle/gpu-alpha?tab=readme-ov-file)

### Articles

- [GPU Programming](https://enccs.github.io/gpu-programming/)
- [History of NVIDIA Stream Multiprocessor](https://fabiensanglard.net/cuda/)
- [Parallel Thread Execution](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)
- [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)
- [CUDA Matrix Multiplication Optimization](https://leimao.github.io/article/CUDA-Matrix-Multiplication-Optimization/)

### Tutorials

- [Intro to Parallel Progamming](https://developer.nvidia.com/udacity-cs344-intro-parallel-programming)

### Videos

- [How GPU Computing Works](https://www.youtube.com/watch?v=3l10o0DYJXg)
- [Getting Started with CUDA for Python Programmers](https://www.youtube.com/watch?v=nOxKexn3iBo)
- [CUDA Training Series](https://www.youtube.com/playlist?app=desktop&list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj&si=ZqKCQgFef-v3JBv8)
- [Programming Parallel Computers](https://www.youtube.com/playlist?list=PL2RY7P3JxZN-Pz1nwvnoJ9uEHmOmv4jmi)
- [GPU Programming](https://www.youtube.com/playlist?list=PL3xCBlatwrsXCGW4SfEoLzKiMSUCE7S_X)
- [CUDA from Scratch](https://www.youtube.com/playlist?list=PLxNPSjHT5qvvwoy6KXzUbLaF5A8NdJvuo)

### Books

- [RDNA3 Instruction Set Architecture](https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf)

## Designing a VM for GPUs

Designing a virtual machine (VM) specifically optimized for GPUs and AI operations is not just a sensible idea, but it's also a direction in which some sectors of the tech industry are heading. Let's explore why this is a compelling notion and what features or characteristics such a VM might possess.

### Rationale:

1. **Specialized Operations**: AI operations, especially deep learning, involve a lot of matrix multiplications and tensor operations. Traditional VMs are not optimized for these.
2. **Massive Parallelism**: GPUs are inherently parallel and can handle thousands of threads simultaneously. A VM designed with this in mind would cater to this strength.
3. **Memory Management**: Deep learning models, especially larger ones, can consume significant amounts of memory. Effective and efficient memory management tailored for these operations can greatly boost performance.
4. **Interoperability**: AI frameworks and libraries are vast and varied (TensorFlow, PyTorch, ONNX, etc.). A VM optimized for AI could provide native support or efficient bridges for these frameworks.

### Features/Characteristics:

1. **Tensor Operations**: First-class support for tensor operations would be a fundamental feature. This means native operations for matrix multiplications, convolutions, etc., optimized for the VM.
2. **Dynamic Scheduling**: Given the parallel nature of GPUs, the VM should be capable of dynamically scheduling tasks based on GPU workloads, ensuring maximum hardware utilization.
3. **Memory Hierarchies**: GPUs have various memory types (global, shared, local, constant). The VM should manage these efficiently, allocating memory based on access patterns and tensor sizes.
4. **Custom Instruction Sets**: The VM's bytecode or intermediate representation might have custom instructions tailored for common AI patterns and operations.
5. **Just-In-Time (JIT) Compilation**: To squeeze out maximum performance, the VM could utilize JIT compilation, translating bytecode to GPU instructions on the fly, with optimizations based on the actual data and model being processed.
6. **Hardware Acceleration**: Besides standard GPU operations, there could be support for AI-specific hardware accelerators or ASICs (like Google's TPUs).
7. **Native Libraries**: Integrated support for popular deep learning libraries or primitives, ensuring optimized performance without redundant operations.
8. **Safety and Security**: With AI models being deployed in critical applications, features to ensure model integrity, data privacy, and secure execution would be essential.
9. **Portability**: One of the core benefits of a VM is portability. The VM should ensure that AI models run consistently across various GPU architectures and versions.
10. **Interoperability Bridges**: Efficient connectors or bridges to interact with popular AI frameworks, ensuring smooth deployment without significant model rewrites.
11. **Debugging and Profiling**: Built-in tools for performance profiling, debugging, and visualization, catering specifically to AI operations.

In essence, the idea of a virtual machine optimized for GPUs and AI operations seeks to bridge the gap between high-level AI frameworks and the raw power of GPU hardware. While creating such a VM is a non-trivial task and demands deep integration between software and hardware, the potential benefits in terms of performance, security, and portability make it a compelling direction for the future of AI infrastructure.

There isn't a mainstream "virtual machine" tailored explicitly for GPUs in the same way that the Java Virtual Machine (JVM) or the Ethereum Virtual Machine (EVM) are for their respective domains. However, there are platforms and runtime environments optimized for GPU operations, especially in the context of AI/ML.

Some of these platforms and their unique features include:

1. **CUDA (Compute Unified Device Architecture)**:
    - **Developer & Manufacturer**: NVIDIA
    - **Unique Features**:
        - Provides a parallel computing platform and API model.
        - Incorporates the CUDA Instruction Set Architecture (ISA) and the parallel compute engine in NVIDIA GPUs.
        - Supports various high-level programming languages like C, C++, and Fortran.
    - **Use Cases**: General GPU-accelerated computing, scientific simulations, deep learning training & inference.
    
2. **TensorRT**:
    - **Developer & Manufacturer**: NVIDIA
    - **Unique Features**:
        - High-performance deep learning inference optimizer and runtime.
        - Reduces the model size for faster inferencing.
        - Provides INT8 and FP16 optimizations for production deployments.
    - **Use Cases**: Optimizing and deploying deep learning models for production environments, especially on NVIDIA GPUs.

3. **ROCm (Radeon Open Compute)**:
    - **Developer & Manufacturer**: AMD
    - **Unique Features**:
        - Open-source platform for GPU-enabled HPC (High Performance Computing) computing.
        - Supports a range of machine learning frameworks like TensorFlow and PyTorch.
    - **Use Cases**: GPU-accelerated computing on AMD GPUs, scientific simulations, AI/ML training & inference.

4. **DirectML**:
    - **Developer**: Microsoft
    - **Unique Features**:
        - Part of the DirectX family, providing a low-level API for ML.
        - Hardware-accelerated and works with any DirectX 12-compatible GPU.
    - **Use Cases**: GPU-accelerated ML inference in Windows applications.

These platforms are more akin to runtime environments than traditional "virtual machines." Still, they are optimized for GPU operations, and they play a role similar to VMs by providing a bridge between high-level code and low-level hardware instructions.

**Main Benefits of GPU-optimized Platforms**

1. **Performance**: Accelerate computations that can be parallelized, dramatically reducing the time required for tasks like training deep learning models.
2. **Efficiency**: Better utilization of available GPU resources, ensuring that the hardware's full capabilities are harnessed.
3. **Portability**: In some cases (like CUDA to some extent), the platform provides an abstraction over the hardware, allowing code to run on different GPU architectures (within limits and considering vendor lock-ins).
4. **Integration with Popular Tools**: Many of these platforms offer integrations with popular ML frameworks, making it easier to transition from research to production.

However, it's essential to understand that while these platforms provide the tools and runtime for GPU-optimized operations, they don't function as "virtual machines" in the traditional sense of emulating a complete computing environment. They are closer to specialized compilers and runtime environments.

## GPUs and RISC

GPUs (Graphics Processing Units) have a different historical design lineage than CPUs. While CPUs are designed for general-purpose tasks and can handle a wide variety of operations in sequence, GPUs are designed for parallelism, optimized for executing the same operation on a large number of data elements simultaneously. This design is especially suitable for graphics rendering tasks, where, for instance, the same shading operation might be applied to millions of pixels.

That said, modern GPUs have evolved and can handle more general-purpose tasks. This field, known as GPGPU (General-Purpose computing on Graphics Processing Units), has grown in prominence, especially with applications in scientific computing, machine learning, etc. 

The instruction set architectures for GPUs are typically proprietary to the manufacturer:

- NVIDIA GPUs use an architecture known as **CUDA** (Compute Unified Device Architecture).
- AMD GPUs use architectures like **GCN** (Graphics Core Next) or RDNA.

While these architectures might share some high-level concepts with RISC — like the importance of efficiency and streamlined operations — they're not RISC architectures in the traditional sense.

### RISC-V & GPUs

RISC-V, given its open nature, is seeing exploration beyond traditional CPUs. There are projects and discussions around using RISC-V principles in GPU design or other specialized hardware accelerators. However, as of my last training data in September 2021, RISC-V's primary traction is in the CPU space, from microcontrollers to more powerful processors.

In conclusion, while RISC and RISC-V are primarily associated with CPUs, the principles and architectures are versatile. The GPU world has its architectures optimized for its specific tasks, but the lines between these domains are evolving, and there's potential for overlap in the future.

