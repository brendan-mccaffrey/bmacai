---
title: Artificial Intelligence
table-of-contents: true
--- 

## Philosophy

- [Asymptotically Unambitious Artificial General Intelligence](https://arxiv.org/abs/1905.12186)
- We Still Don't Understand the Models We've Built
  - ["Glitch Token"](https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)

## Uncategorized Resources

- [Retreival-Enhanced Large Language Models](https://arxiv.org/abs/2301.00303)
    - [Tweet](https://twitter.com/omarsar0/status/1610469384224473088?s=20&t=ooKBDOSX4f3N4fzPDNLjSw)
- [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618)
- [High Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- [Multimodal Neurons in Artificial Neural Networks](https://distill.pub/2021/multimodal-neurons/)
- [Parsel: A Unified Natural Language Framework for Algorithmic Reasoning](https://arxiv.org/abs/2212.10561)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) 
    - Google investigates optimal training for language models, finds that models are undertrained (i.e. data is the bottleneck, not compute).
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/pdf/2201.02177.pdf) 
    - OpenAI & Google: Improving to perfect generalization well past overfitting.
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
- [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)
- [Pseudo Numerical Methods for Diffusion Models on Manifolds](https://arxiv.org/abs/2202.09778)
- [Fast Sampling of Diffusion Models with Exponential Integrator](https://arxiv.org/abs/2204.13902)
- [Broken Neural Scaling Laws](https://arxiv.org/pdf/2210.14891.pdf) 
    - Modelling scaling behaviors of deep neural networks.
- [Deepmind: Discovering novel algorithms with AlphaTensor](https://www.nature.com/articles/s41586-022-05172-4) 
    - Discovering faster matrix multiplication algorithms with reinforcement learning.
    - [Blog](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)
- [Deepmind: Accelerating fusion science through learned plasma control](https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control)
- [PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning](https://arxiv.org/abs/2205.07000)
- [Large Language Models can Self-Improve](https://arxiv.org/abs/2210.11610)
- [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)
- [Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
- [Meta: CICERO](https://ai.facebook.com/research/cicero/) 
    - The first AI to play at a human level in Diplomacy, a strategy game that requires building trust, negotiating and cooperating with multiple players.
- [OpenAI: Learning to play Minecraft with video pre-training (VPT)](https://openai.com/blog/vpt/)
- [DALL-E: Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092)
- [SparseGPT](https://arxiv.org/abs/2301.00774)     
    - GPT-family models can be pruned 50%+ sparsity in one-shot, without retraining and minimal loss of accuracy.
- [Make-A-Video: Text-to-Video Generation without Text-Video Data](https://arxiv.org/abs/2209.14792)
- [Galactica: A Large Language Model for Science](https://galactica.org/static/paper.pdf)
- [OPT-IML: An instruction-tuned LLM for open-use](https://t.co/ZFaP4PXTVv)
    - [Github](https://t.co/H9reL14LR8)
- [Dreambooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) 
    - mitigates the coherence problem in image generation (consistent generations) by binding a unique identifier to a fine-tuned model. This method powers the AI avatar creators, game asset generators, AI movies, and more.
- [Deep Learning and Computational Physics (Lecture Notes)](https://arxiv.org/abs/2301.00942v1)
- [Diffusion Language Models](https://benanne.github.io/2023/01/09/diffusion-language.html)
- [Does Compressing Activations Help Model Parallel Training?](https://t.co/xecl0lOlo1)
- [VALL-E: Generate voice snippets with only a few seconds of input audio](https://arxiv.org/abs/2301.02111)
    - [Website](https://valle-demo.github.io/)
- [Wolfram Alpha x ChatGPT](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)
- [Data Distillation](https://t.co/qZBhgSofuX)
- [Adjusting Biased Samples](https://t.co/yVKBfdjAZa) 
    - Easy to use framework for weighting data and evaluating its biases with and without adjustments.
    - [Github](https://t.co/bDJw0ix2zN)
- [Circumvent Context Size Limits](https://t.co/1RcV8fhUSW)
- [LLM Refinement Process, reduce mistakes](https://twitter.com/cwolferesearch/status/1613643034717028352)
- [Multimodal Deep Learning](https://t.co/zVnIcSwMIW)
- [A Deep-Learning-Based Multi-Modal Sensor Fusion Approach for Detection of Equipment Faults](https://www.mdpi.com/2075-1702/10/11/1105)
- [Whisper: OpenAI Speech Recognition](https://arxiv.org/abs/2212.04356)
    - [blog](https://openai.com/blog/whisper/)
    - [github](https://github.com/openai/whisper)
- [ChatGPT Political Biases](https://davidrozado.substack.com/p/political-bias-chatgpt)
- [Pix2Pix](https://arxiv.org/pdf/2211.09800.pdf)
    - [github](https://github.com/timothybrooks/instruct-pix2pix)
- [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)
    - [tweet](https://twitter.com/zacharynado/status/1616133152438910976)
- [3D Image Inpainting](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/3D_Photo_Inpainting_multiple_download.ipynb#scrollTo=5o-EIMeaghU0)
    - [tweet](https://twitter.com/originalmaderix/status/1595770992495710209?s=20&t=fwFT1N12RsVrqI9onY4rRQ)
- [Detecting Watermarks on GPT](https://twitter.com/tomgoldsteincs/status/1618287665006403585?s=20&t=M3iFtvAjzAXtua_e1l-WzQ)
- [Transformers: How "emergent abilities" are unlocked by scaling up language models](https://t.co/Bqht9OEw7m)
    - [tweet](https://twitter.com/_jasonwei/status/1618331876623523844?s=20&t=0Lv9OBeHcPZ7drotXNbVyg)
- [Google MusicLM](https://twitter.com/nonmayorpete/status/1618996491649171457?s=20&t=zYURx0HbIgrKRcnPSRb4MQ)
- [HuggingFace Stable Diffusion: Text2Img, Img2Img](https://huggingface.co/spaces/camenduru/webui)
- [BLIP-2: Image to Text](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
    - [tweet](https://twitter.com/LiJunnan0409/status/1621649677543440384?s=20&t=OdX-rH843kQC2bO6FvbRdw)
    - [huggingface](https://huggingface.co/spaces/Salesforce/BLIP2)
- [Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)
- [Google | Introducing Gemini: our largest and most capable AI model](https://blog.google/technology/ai/google-gemini-ai/)
- [Liquid AI](https://www.liquid.ai/)
- [Learning Universal Predictors](https://arxiv.org/pdf/2401.14953.pdf)

