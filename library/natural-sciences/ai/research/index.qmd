---
title: Research
description: Unorganized list of AI Research
---

## Resources

- [Retreival-Enhanced Large Language Models](https://arxiv.org/abs/2301.00303)
    - [Tweet](https://twitter.com/omarsar0/status/1610469384224473088?s=20&t=ooKBDOSX4f3N4fzPDNLjSw)
- [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618)
- [High Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- [Multimodal Neurons in Artificial Neural Networks](https://distill.pub/2021/multimodal-neurons/)
- [Parsel: A Unified Natural Language Framework for Algorithmic Reasoning](https://arxiv.org/abs/2212.10561)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) 
    - Google investigates optimal training for language models, finds that models are undertrained (i.e. data is the bottleneck, not compute).
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/pdf/2201.02177.pdf) 
    - OpenAI & Google: Improving to perfect generalization well past overfitting.
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
- [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)
- [Pseudo Numerical Methods for Diffusion Models on Manifolds](https://arxiv.org/abs/2202.09778)
- [Fast Sampling of Diffusion Models with Exponential Integrator](https://arxiv.org/abs/2204.13902)
- [Broken Neural Scaling Laws](https://arxiv.org/pdf/2210.14891.pdf) 
    - Modelling scaling behaviors of deep neural networks.
- [Deepmind: Discovering novel algorithms with AlphaTensor](https://www.nature.com/articles/s41586-022-05172-4) 
    - Discovering faster matrix multiplication algorithms with reinforcement learning.
    - [Blog](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)
- [Deepmind: Accelerating fusion science through learned plasma control](https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control)
- [PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning](https://arxiv.org/abs/2205.07000)
- [Large Language Models can Self-Improve](https://arxiv.org/abs/2210.11610)
- [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)
- [Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
- [OpenAI: Learning to play Minecraft with video pre-training (VPT)](https://openai.com/blog/vpt/)
- [SparseGPT](https://arxiv.org/abs/2301.00774)     
    - GPT-family models can be pruned 50%+ sparsity in one-shot, without retraining and minimal loss of accuracy.
- [Diffusion Language Models](https://benanne.github.io/2023/01/09/diffusion-language.html)
- [Does Compressing Activations Help Model Parallel Training?](https://t.co/xecl0lOlo1)
- [Data Distillation](https://t.co/qZBhgSofuX)
- [Adjusting Biased Samples](https://t.co/yVKBfdjAZa) 
    - Easy to use framework for weighting data and evaluating its biases with and without adjustments.
    - [Github](https://t.co/bDJw0ix2zN)
- [Circumvent Context Size Limits](https://t.co/1RcV8fhUSW)
- [LLM Refinement Process, reduce mistakes](https://twitter.com/cwolferesearch/status/1613643034717028352)
- [Multimodal Deep Learning](https://t.co/zVnIcSwMIW)
- [A Deep-Learning-Based Multi-Modal Sensor Fusion Approach for Detection of Equipment Faults](https://www.mdpi.com/2075-1702/10/11/1105)
- [Detecting Watermarks on GPT](https://twitter.com/tomgoldsteincs/status/1618287665006403585?s=20&t=M3iFtvAjzAXtua_e1l-WzQ)
- [Transformers: How "emergent abilities" are unlocked by scaling up language models](https://t.co/Bqht9OEw7m)
    - [tweet](https://twitter.com/_jasonwei/status/1618331876623523844?s=20&t=0Lv9OBeHcPZ7drotXNbVyg)
- [Google MusicLM](https://twitter.com/nonmayorpete/status/1618996491649171457?s=20&t=zYURx0HbIgrKRcnPSRb4MQ)
- [HuggingFace Stable Diffusion: Text2Img, Img2Img](https://huggingface.co/spaces/camenduru/webui)
- [BLIP-2: Image to Text](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
    - [tweet](https://twitter.com/LiJunnan0409/status/1621649677543440384?s=20&t=OdX-rH843kQC2bO6FvbRdw)
    - [huggingface](https://huggingface.co/spaces/Salesforce/BLIP2)
- [Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)
- [Beyond $A^*$: Better Planning with Transformers via Search Dynamics Bootstrapping](https://arxiv.org/pdf/2402.14083.pdf)