---
title: Research
description: Unorganized list of AI Research
---

## Resources

- [Retreival-Enhanced Large Language Models](https://arxiv.org/abs/2301.00303)
    - [Tweet](https://twitter.com/omarsar0/status/1610469384224473088?s=20&t=ooKBDOSX4f3N4fzPDNLjSw)
- [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618)
- [High Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- [Multimodal Neurons in Artificial Neural Networks](https://distill.pub/2021/multimodal-neurons/)
- [Parsel: A Unified Natural Language Framework for Algorithmic Reasoning](https://arxiv.org/abs/2212.10561)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) 
    - Google investigates optimal training for language models, finds that models are undertrained (i.e. data is the bottleneck, not compute).
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/pdf/2201.02177.pdf) 
    - OpenAI & Google: Improving to perfect generalization well past overfitting.
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
- [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)
- [Pseudo Numerical Methods for Diffusion Models on Manifolds](https://arxiv.org/abs/2202.09778)
- [Fast Sampling of Diffusion Models with Exponential Integrator](https://arxiv.org/abs/2204.13902)
- [Broken Neural Scaling Laws](https://arxiv.org/pdf/2210.14891.pdf) 
    - Modelling scaling behaviors of deep neural networks.
- [Deepmind: Discovering novel algorithms with AlphaTensor](https://www.nature.com/articles/s41586-022-05172-4) 
    - Discovering faster matrix multiplication algorithms with reinforcement learning.
    - [Blog](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)
- [Deepmind: Accelerating fusion science through learned plasma control](https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control)
- [PrefixRL: Optimization of Parallel Prefix Circuits using Deep Reinforcement Learning](https://arxiv.org/abs/2205.07000)
- [Large Language Models can Self-Improve](https://arxiv.org/abs/2210.11610)
- [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)
- [Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
- [OpenAI: Learning to play Minecraft with video pre-training (VPT)](https://openai.com/blog/vpt/)
- [SparseGPT](https://arxiv.org/abs/2301.00774)     
    - GPT-family models can be pruned 50%+ sparsity in one-shot, without retraining and minimal loss of accuracy.
- [Diffusion Language Models](https://benanne.github.io/2023/01/09/diffusion-language.html)
- [Does Compressing Activations Help Model Parallel Training?](https://t.co/xecl0lOlo1)
- [Data Distillation](https://t.co/qZBhgSofuX)
- [Adjusting Biased Samples](https://t.co/yVKBfdjAZa) 
    - Easy to use framework for weighting data and evaluating its biases with and without adjustments.
    - [Github](https://t.co/bDJw0ix2zN)
- [Circumvent Context Size Limits](https://t.co/1RcV8fhUSW)
- [LLM Refinement Process, reduce mistakes](https://twitter.com/cwolferesearch/status/1613643034717028352)
- [Multimodal Deep Learning](https://t.co/zVnIcSwMIW)
- [A Deep-Learning-Based Multi-Modal Sensor Fusion Approach for Detection of Equipment Faults](https://www.mdpi.com/2075-1702/10/11/1105)
- [Detecting Watermarks on GPT](https://twitter.com/tomgoldsteincs/status/1618287665006403585?s=20&t=M3iFtvAjzAXtua_e1l-WzQ)
- [Transformers: How "emergent abilities" are unlocked by scaling up language models](https://t.co/Bqht9OEw7m)
    - [tweet](https://twitter.com/_jasonwei/status/1618331876623523844?s=20&t=0Lv9OBeHcPZ7drotXNbVyg)
- [Google MusicLM](https://twitter.com/nonmayorpete/status/1618996491649171457?s=20&t=zYURx0HbIgrKRcnPSRb4MQ)
- [HuggingFace Stable Diffusion: Text2Img, Img2Img](https://huggingface.co/spaces/camenduru/webui)
- [BLIP-2: Image to Text](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
    - [tweet](https://twitter.com/LiJunnan0409/status/1621649677543440384?s=20&t=OdX-rH843kQC2bO6FvbRdw)
    - [huggingface](https://huggingface.co/spaces/Salesforce/BLIP2)
- [Meta: CICERO](https://ai.facebook.com/research/cicero/) 
    - The first AI to play at a human level in Diplomacy, a strategy game that requires building trust, negotiating and cooperating with multiple players.
- [Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)
- [Beyond $A^*$: Better Planning with Transformers via Search Dynamics Bootstrapping](https://arxiv.org/pdf/2402.14083.pdf)
- [Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)
- [Extreme Compression of LLMs via Additive Quantization](https://arxiv.org/abs/2401.06118)
- [Distributed Inference and Fine-tuning of LLMs over the Internet](https://browse.arxiv.org/html/2312.08361v1)
- [Distributed SLIDE: Enabling Training Large Neural Network on Low Bandwidth and Simple CPU-Clusters via Model Parallelism and Sparsity](https://arxiv.org/pdf/2201.12667.pdf)
- [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020)](http://arxiv.org/abs/2005.11401)
- [LoRA: Low-Rank Adaptation of Large Language Models (2021)](http://arxiv.org/abs/2106.09685)
- [Ring Attention with Blockwise Transformers for Near-Infinite Context (2023)](http://arxiv.org/abs/2310.01889)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](http://arxiv.org/abs/2310.01889)
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](http://arxiv.org/abs/2402.17764)
- [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](http://arxiv.org/abs/2403.09629)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](http://arxiv.org/abs/2404.07143)
- [The Illusion of State in State-Space Models](http://arxiv.org/abs/2404.08819)
- [Ïƒ-GPTs: A New Approach to Autoregressive Models](http://arxiv.org/abs/2404.09562)
- [Rethinking LLM Memorization through the Lens of Adversarial Compression](http://arxiv.org/abs/2404.15146)
- [Better & Faster Large Language Models via Multi-token Prediction](http://arxiv.org/abs/2404.19737)
- [xLSTM: Extended Long Short-Term Memory](http://arxiv.org/abs/2405.04517)
- [Chameleon: Mixed-Modal Early-Fusion Foundation Models](http://arxiv.org/abs/2405.09818)
- [Transformers Can Do Arithmetic with the Right Embeddings](http://arxiv.org/abs/2405.17399)
- [Scalable MatMul-free Language Modeling](http://arxiv.org/abs/2406.02528)
