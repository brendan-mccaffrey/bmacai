---
title: Transformers
---

## Resources

- [Let's Build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention is all you need, video paper](https://www.youtube.com/watch?v=iDulhoQ2pro)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [GPT in 60 Lines of Numpy](https://jaykmody.com/blog/gpt-from-scratch/)
- Gen AI Visualization by FT: [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)
- [Illustrated Guide to Transformers](https://www.youtube.com/watch?v=4Bdc55j80l8)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Transformers from Scratch](https://e2eml.school/transformers.html)

### The Introduction: "Attention is All You Need"

Paper published by a team at Google Brain.

- [Attention is All You Need](https://arxiv.org/abs/1706.03762)

The Transformer architecture is built on the idea of self-attention or scaled dot-product attention, which can be seen as an extension of the attention mechanism. In traditional sequence-to-sequence models, such as recurrent neural networks (RNNs), sequential information is processed step by step, which can be computationally expensive and hinder parallelization.

The key innovation of the Transformer is that it replaces recurrent layers with self-attention layers. Self-attention allows the model to compute the relationships between all positions in the input sequence simultaneously. This means that each position can attend to any other position, capturing global dependencies in the sequence. By doing so, the Transformer can process inputs in parallel and handle long-range dependencies more effectively.

The Transformer architecture consists of two main components: the encoder and the decoder. The encoder takes an input sequence and generates a sequence of contextualized representations, while the decoder takes the encoder's output and generates the final output sequence.

Within the encoder and decoder, the Transformer uses multi-head self-attention mechanisms. Instead of a single self-attention mechanism, multiple attention heads are employed to capture different types of information and attend to different parts of the input sequence simultaneously. This allows the model to focus on different aspects of the input and learn more diverse and expressive representations.

The Transformer also introduces positional encoding, which provides information about the position of each element in the input sequence. Since self-attention lacks the notion of position, positional encoding enables the model to incorporate sequential information into its computations.

The dominance of the Transformer architecture in modern AI can be attributed to several factors:

1. Parallelization: The self-attention mechanism in the Transformer enables parallel processing of inputs, making it highly efficient for training on modern hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs).

2. Long-range dependencies: The self-attention mechanism allows the Transformer to capture dependencies between distant positions in the input sequence, making it more effective in modeling long-range relationships compared to traditional recurrent architectures.

3. Expressiveness: By employing multiple attention heads and capturing different types of information, the Transformer can learn more expressive and rich representations of the input data.

4. Transferability: The Transformer has shown strong performance across a wide range of NLP tasks, such as machine translation, language understanding, text generation, and more. Its effectiveness and versatility have made it a popular choice and a basis for many state-of-the-art models.

Overall, the Transformer's ability to model global dependencies, parallelize computations, and learn expressive representations has contributed to its dominance in modern AI, particularly in the field of natural language processing.
