---
title: Attention
---

### Attention: A Huge Leap

In 2014, an Attention mechanism was introduced by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. This became the foundation for the Transformer.
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [ffective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)

The attention mechanism allows the model to assign different weights or importance to different parts of the input sequence when making predictions or generating output. Instead of relying solely on fixed-length vector representations or encoding the entire input sequence into a fixed-length representation, the attention mechanism enables the model to dynamically attend to relevant parts of the input at each step of the computation.

In a typical attention mechanism, there are three main components:

1. Query: A query represents the current step or position in the output generation process. It is used to compare against different parts of the input sequence to determine their relevance.

2. Key: The keys are representations of the different parts of the input sequence. These keys are compared to the query to measure their similarity or relevance.

3. Value: The values are additional information associated with the input sequence parts. They provide the context or content that the model focuses on.

The attention mechanism computes a similarity score between the query and each key, which determines the relevance or attention assigned to the corresponding value. These similarity scores are typically computed using dot products, additive or multiplicative operations, or neural networks.

The resulting attention weights are used to weight the values associated with the input sequence parts. This weighted information is then combined or aggregated to provide a context-aware representation for the current step, allowing the model to focus more on relevant parts of the input.

By incorporating attention mechanisms into deep learning models, they can effectively capture dependencies between different elements of the input sequence, improve performance on sequential tasks, and handle long-range dependencies more effectively than traditional fixed-length representations.
