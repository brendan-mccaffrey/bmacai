---
title: Context Windows
---

### The Evolution of Context Windows

##### **Introduction**

The context window—the maximum sequence length that a model can process in a single pass—is a hot topic in the field of large language models. It serves as a critical constraint for tasks that require understanding or generating longer sequences of text. From OpenAI's ambitious goal of one million tokens to Magic's announcement of a 5M token model, and Anthropic's 100K context window model, we're witnessing a context-length party. But why stop there? This article explores novel techniques and architectures that aim to stretch or even potentially eliminate these limitations.

##### **TL;DR**

- Context window length limitations have been a bottleneck in the performance of large language models.
- Various tricks like SuperHOT, and architectures like RWKV and Hyena, are breaking these limitations.
- Each approach comes with its trade-offs, but the direction is clear: larger context windows are inevitable.

##### **The SuperHOT Method**

About two weeks ago, an innovative trick called SuperHOT surfaced on 4-chan. The user claimed that dividing the model's positional encoding by 4 could trick the model into working with lengths 4 times longer than its original setting. The trick gained enough traction to be cited in academic papers. Following this, an anonymous Reddit user improved upon the idea, using non-linear interpolation to further enhance the performance of these "stretched" models. However, such tricks are not without drawbacks; performance dips and hyperparameter tuning are among the issues.

##### **Infinite Context with RWKV**

RWKV, known as the largest LSTM network in the world, is unique in its ability to theoretically offer an infinite context length. It ranks among the five best models in the Vicuna table and is one of the most used models on Huggingface. It's not just good—it's also cheap to run, operating at a lower computational cost than its Transformer counterparts.

##### **GPT-4: A Tangent on Architecture**

The latest in the line of OpenAI's GPT models, GPT-4 is a behemoth with eight different transformers and 220 billion parameters. Its architecture has sparked theories and speculation, some even suggesting that the model can dynamically change while running. It remains a remarkable but expensive framework, leading us to consider alternative architectures that can handle longer sequences more efficiently.

##### **Goodbye Transformers, Hello Hyena**

Enter Hyena, a convolution-based architecture that recently outperformed a larger Transformer model. It promises monstrous sequence lengths, faster computation, and possibly even higher accuracy. Hyena's secret sauce is its ability to learn a map from position encoding to filter values, enabling more context-aware processing without the computational intensity of Transformers.

###### **The Core Trick in Hyena**

Hyena generates filters based on positional encoding. These filters are then applied in a Toeplitz matrix, which mimics the function of a convolution operation. This unique architecture allows Hyena to be as "data-controlled" as possible, similar to how attention layers work in Transformers but at a fraction of the computational cost.

##### **Conclusion**

From tweaking positional encodings in the SuperHOT method to adopting entirely different architectures like RWKV and Hyena, the pursuit of longer context windows in large language models is more alive than ever. As we move forward, we can expect even more groundbreaking innovations that push the boundaries, opening new possibilities for applications that require understanding vast swaths of data in a single glance.

---

**Further Reading**

- [GLam: Mixture of Experts](https://arxiv.org/pdf/2112.06905.pdf)
- [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf)
- [Routed Language Models](https://arxiv.org/pdf/2202.01169.pdf)
- [Branch-Train-Merge](https://arxiv.org/pdf/2208.03306.pdf)
- [Scaling Laws of Expert Networks](https://arxiv.org/pdf/2202.01169.pdf)
- [Combining Knowledge Domains in MoE Nets](https://arxiv.org/pdf/2303.14177.pdf)
- [Hyena Paper](https://arxiv.org/abs/2302.10866)
- [Hyena Blogs](https://hazyresearch.stanford.edu/blog)

---