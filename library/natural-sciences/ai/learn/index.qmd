---
title: Learn AI
---

### Grand Level Overview

- [Probabalistic Machine Learning](https://probml.github.io/pml-book/book1.html)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [ProbML by Kevin Murphy](https://probml.github.io/pml-book/)
- [Geoffrey Hinton, Intro to NNs and ML, CSC321](https://www.dropbox.com/scl/fi/7bzpik2hhblb431cnfe9a/hintonlecturesmerged.pdf?rlkey=r0kht5s1byqswslrt125dkope&dl=0)
- [Little Book of Deep Learning](https://fleuret.org/public/lbdl.pdf)

## Lecture Series

- [Stanford Machine Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy) and corresponding [notes](https://cs229.stanford.edu/main_notes.pdf)
- [Stanford Transformers](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)
- [Stanford generative models including diffusion](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8)
- [Stanford Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)
- [Karpathy Neural Networks Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
- [Stanford NLP with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4)
- [MIT Deep Learning](https://www.youtube.com/playlist?list=PLTZ1bhP8GBuTCqeY19TxhHyrwFiot42_U)
- [Stanford Meta Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI)
- [Stanford Methods in AI](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX)

### Maths

- [Gradient Descent does not Always Converge](https://twitter.com/gabrielpeyre/status/1750760482028519635)


### Backpropogation

Backpropogation is an algorithm for training neural-networks, used to update its weights to minimize the 'loss'

1. Network makes a prediction on a batch of input data
2. Loss is calculated between predicted and actual output
3. Gradient of the loss with respect to the weights are calculated using the chain rule of differentiation
4. The gradients are used to update the weights in the opposite direction of the gradient, reducing the loss
5. Repeat the process until the loss reaches a satisfactory level or a maximum number of iterations

