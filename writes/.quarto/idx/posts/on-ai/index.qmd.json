{"title":"On the Risks of Artificial Intelligence","markdown":{"yaml":{"title":"On the Risks of Artificial Intelligence","image":"image.jpeg","author":"Brendan McCaffrey","date":"2022-12-16","sidebar":false},"headingText":"Background","containsRefs":false,"markdown":"\n\n\nI've spent a some time meditating on this question. I have interacted with the industry as a consumer, as an investor, as a startup contributor, and as a (novice) developer. Most importantly, I've had fruitful discussions with, and have listened very carefully to, many people with more perspectives than I.\n\nThat's not to say my views are necessarily important, or even correct - it's just a bit of context. I remain open-minded, and I welcome discussion, counter-points, and new information...\n\nI see two main, distinct vectors of risk\n\n1. Loss of Human Dominance\n2. Centralization of Information Flow\n\n### Loss of Human Dominance\n\nThe first and most focused on vector of risk is the danger of a(n) agent(s) which surpass(es) humans in intelligence & aptitude. This is the rogue AGI scenario. Humans have held the top spot for ~300,000 years, and no one is capable of predicting what would happen if/when that changes.\n\nThis is the vector addressed by \"Pause Giant AI Experiments: An Open Letter\" \nhttps://futureoflife.org/open-letter/pause-giant-ai-experiments/\n\nI believe this vector of risk is real, and it is tremendously dangerous.\n\nCybersecurity will become very tricky.\nhttps://twitter.com/0xbmac/status/1664668707820273669?s=20\n\nMilitary applications will introduce existential threats\nhttps://academic.oup.com/ijlit/article-pdf/30/4/472/49978077/eaad008.pdf\n\nDomestic use cases (surveillance, policing) will yield tremendous controversy\n\nIn my opinion, the day-to-day news stories/headlines draw too much attention and cause us to overestimate the short-term risks. Yet, the longer term horizon of this paradigm remain wildly under-appreciated.\n\nRegardless of my (or anyone's) view on the severity of this risk vector, it's naive to think a \"pause\" on AI will ever happen. \n\nThis is an arms race.\n\n### Centralization of Information Flow\n\nThe second vector of risk, which I believe is more relevant in the short-term, regards what's being referred to as the \"Moderation Layer\". \n\nThis risk is not about the AI agent itself. Rather, it concerns the humans & organizations that own these models and are responsible for regulating (or in the position to regulate) usage. Although discussion on the correct approach to regulation/moderation takes place, these often happen in the political realm, wherein a strictly two-dimensional tug-of-war stagnates.\n\nMain Question: \nWhat is the goal of the moderation layer, and why is it necessary?\n\n> \"To censor dangerous content\"\n- What exactly defines \"dangerous\"?\n- Would a blanket definition of \"dangerous\" be acceptable? For example, allowing an AI explain how to enrich Uranium to a jihadist group can likely be considered dangerous. Allowing the AI to explain this to a group of PhD Chemical Engineers, on the other hand, is not as dangerous.\n\n> \"To censor offensive content\"\n- All statements are offensive to some extent, so where exactly are we drawing the line? To what extent should we be tolerant in the name of free speech? \"Offensive\" is a spectrum - there is no statement that offends none. It becomes clear to ask, to what extent are we willing to restrict the freedom of thought to protect users from being offended?\n\n> \"To limit the spread of misinformation\"\n- I have a appreciation for the harm that AI can do with respect to proliferating false information. But if the past few years have taught us anything, it's that \"misinformation\" is now synonymous with \"I don't like that information\".\n\nI am not going to assert my conclusions on these follow-ups to the main question. I believe there are no objective answers to these questions. Thus, they must be governed by a diverse body of humans.\n\nLarge language models (LLMs) like OpenAI's GPT-4 are trained on billions of lines of text, extracted from the internet, from millions of (human) sources of various demographics, representing a vast array of perspectives.\n\nOpenAI's moderation layer is built by less than 100 humans, nearly all of whom have a similar background and reside in Silicon Valley.\n\n\"The political ideology of conversational AI\"\nhttps://arxiv.org/abs/2301.01768\n\nTo believe the latter is capable of mitigating the bias the former, in my humble opinion, is simply incorrect.\n\nGPT-4 System Card, the Appendix\nhttps://cdn.openai.com/papers/gpt-4-system-card.pdf\n\nI am not claiming the former (the agent) is unbiased. LLMs are trained heavily on text from Reddit, for example, which has an overrepresentation of men than women. Further, as (almost) all training data is extracted from the internet, the information from which the AI model \"learns\" is inherently biased towards the digital world: internet users, citizens of first world countries, etc.\n\nMy current stance is that the moderation layer is necessary, but\n\n1) The implementation details of the moderation layer *must* be transparent.\n\n2) Governance of the moderation layer must be, to some extent, decentralized. At the very least, let us agree the power should not lie solely in the hands of 0.00000125% of the world's population.","srcMarkdownNoYaml":"\n\n### Background\n\nI've spent a some time meditating on this question. I have interacted with the industry as a consumer, as an investor, as a startup contributor, and as a (novice) developer. Most importantly, I've had fruitful discussions with, and have listened very carefully to, many people with more perspectives than I.\n\nThat's not to say my views are necessarily important, or even correct - it's just a bit of context. I remain open-minded, and I welcome discussion, counter-points, and new information...\n\nI see two main, distinct vectors of risk\n\n1. Loss of Human Dominance\n2. Centralization of Information Flow\n\n### Loss of Human Dominance\n\nThe first and most focused on vector of risk is the danger of a(n) agent(s) which surpass(es) humans in intelligence & aptitude. This is the rogue AGI scenario. Humans have held the top spot for ~300,000 years, and no one is capable of predicting what would happen if/when that changes.\n\nThis is the vector addressed by \"Pause Giant AI Experiments: An Open Letter\" \nhttps://futureoflife.org/open-letter/pause-giant-ai-experiments/\n\nI believe this vector of risk is real, and it is tremendously dangerous.\n\nCybersecurity will become very tricky.\nhttps://twitter.com/0xbmac/status/1664668707820273669?s=20\n\nMilitary applications will introduce existential threats\nhttps://academic.oup.com/ijlit/article-pdf/30/4/472/49978077/eaad008.pdf\n\nDomestic use cases (surveillance, policing) will yield tremendous controversy\n\nIn my opinion, the day-to-day news stories/headlines draw too much attention and cause us to overestimate the short-term risks. Yet, the longer term horizon of this paradigm remain wildly under-appreciated.\n\nRegardless of my (or anyone's) view on the severity of this risk vector, it's naive to think a \"pause\" on AI will ever happen. \n\nThis is an arms race.\n\n### Centralization of Information Flow\n\nThe second vector of risk, which I believe is more relevant in the short-term, regards what's being referred to as the \"Moderation Layer\". \n\nThis risk is not about the AI agent itself. Rather, it concerns the humans & organizations that own these models and are responsible for regulating (or in the position to regulate) usage. Although discussion on the correct approach to regulation/moderation takes place, these often happen in the political realm, wherein a strictly two-dimensional tug-of-war stagnates.\n\nMain Question: \nWhat is the goal of the moderation layer, and why is it necessary?\n\n> \"To censor dangerous content\"\n- What exactly defines \"dangerous\"?\n- Would a blanket definition of \"dangerous\" be acceptable? For example, allowing an AI explain how to enrich Uranium to a jihadist group can likely be considered dangerous. Allowing the AI to explain this to a group of PhD Chemical Engineers, on the other hand, is not as dangerous.\n\n> \"To censor offensive content\"\n- All statements are offensive to some extent, so where exactly are we drawing the line? To what extent should we be tolerant in the name of free speech? \"Offensive\" is a spectrum - there is no statement that offends none. It becomes clear to ask, to what extent are we willing to restrict the freedom of thought to protect users from being offended?\n\n> \"To limit the spread of misinformation\"\n- I have a appreciation for the harm that AI can do with respect to proliferating false information. But if the past few years have taught us anything, it's that \"misinformation\" is now synonymous with \"I don't like that information\".\n\nI am not going to assert my conclusions on these follow-ups to the main question. I believe there are no objective answers to these questions. Thus, they must be governed by a diverse body of humans.\n\nLarge language models (LLMs) like OpenAI's GPT-4 are trained on billions of lines of text, extracted from the internet, from millions of (human) sources of various demographics, representing a vast array of perspectives.\n\nOpenAI's moderation layer is built by less than 100 humans, nearly all of whom have a similar background and reside in Silicon Valley.\n\n\"The political ideology of conversational AI\"\nhttps://arxiv.org/abs/2301.01768\n\nTo believe the latter is capable of mitigating the bias the former, in my humble opinion, is simply incorrect.\n\nGPT-4 System Card, the Appendix\nhttps://cdn.openai.com/papers/gpt-4-system-card.pdf\n\nI am not claiming the former (the agent) is unbiased. LLMs are trained heavily on text from Reddit, for example, which has an overrepresentation of men than women. Further, as (almost) all training data is extracted from the internet, the information from which the AI model \"learns\" is inherently biased towards the digital world: internet users, citizens of first world countries, etc.\n\nMy current stance is that the moderation layer is necessary, but\n\n1) The implementation details of the moderation layer *must* be transparent.\n\n2) Governance of the moderation layer must be, to some extent, decentralized. At the very least, let us agree the power should not lie solely in the hands of 0.00000125% of the world's population."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"simplex","title-block-banner":false,"sidebar":false,"title":"On the Risks of Artificial Intelligence","image":"image.jpeg","author":"Brendan McCaffrey","date":"2022-12-16"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}